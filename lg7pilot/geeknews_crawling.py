# geeknews_crawling.py
"""
GeekNews (https://news.hada.io/) í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
ìµœì‹ ê¸€ê³¼ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import pymysql  # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ìš©
import sys
import time
import os
from datetime import datetime
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import re

# Windows í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
os.environ['PYTHONIOENCODING'] = 'utf-8'
sys.stdout.reconfigure(encoding='utf-8')

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì • - íŒ¨ìŠ¤ì›Œë“œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
try:
    conn = pymysql.connect(
        user="lguplus7",
        password=r"ë°œê¸‰ë°›ì€_DB_PASSWORD",  # raw stringìœ¼ë¡œ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
        host="localhost",
        port=3306,
        database="cp_data",
        charset='utf8mb4'
    )
    print("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {e}")
    sys.exit(1)

# ì»¤ì„œ ìƒì„±
cur = conn.cursor()

def save_url_to_ready(url, source_type='1'):
    """
    URLì„ gn_scrap_ready í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
    source_type: '1' = GeekNews
    """
    # ìœ íš¨í•œ GeekNews ì•„ì´í…œ URLì¸ì§€ í™•ì¸
    if not url.startswith('https://news.hada.io/topic?id='):
        print(f"    ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹: {url[:50]}...")
        return False
        
    try:
        # ì¤‘ë³µ ì²´í¬
        cur.execute("SELECT seq_no FROM gn_scrap_ready WHERE source_url = %s", (url,))
        if cur.fetchone():
            print(f"    ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URL: {url[:50]}...")
            return False
        
        # ìƒˆ URL ì €ì¥
        create_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        cur.execute(
            "INSERT INTO gn_scrap_ready (source_type, source_url, status, create_dt) VALUES (%s, %s, '0', %s)",
            (source_type, url, create_time)
        )
        conn.commit()
        print(f"    ìƒˆ URL ì €ì¥ ì„±ê³µ: {url[:50]}...")
        return True
        
    except Exception as e:
        print(f"    URL ì €ì¥ ì˜¤ë¥˜: {e}")
        conn.rollback()  # ì˜¤ë¥˜ ì‹œ ë¡¤ë°±
        return False

def scrape_latest_items():
    """ìµœì‹ ê¸€ í˜ì´ì§€ì—ì„œ ì•„ì´í…œë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    
    print("\nìµœì‹ ê¸€ ìˆ˜ì§‘ ì‹œì‘...")
    
    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        browser = p.firefox.launch(headless=True)
        page = browser.new_page()
        
        try:
            # ìµœì‹ ê¸€ í˜ì´ì§€ë¡œ ì´ë™
            print("í˜ì´ì§€ ë¡œë“œ ì¤‘: https://news.hada.io/new")
            page.goto("https://news.hada.io/new", timeout=30000)
            time.sleep(3)  # ë¡œë´‡ ë°°ì œ í‘œì¤€ ì¤€ìˆ˜ (3ì´ˆ ëŒ€ê¸°)
            
            # í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            content = page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¥¼ ì‹œë„í•´ì„œ ì•„ì´í…œ ì°¾ê¸°
            items = []
            
            # ë°©ë²• 1: topictitle í´ë˜ìŠ¤ê°€ ìˆëŠ” div ì§ì ‘ ì°¾ê¸°
            topic_divs = soup.find_all('div', class_='topictitle')
            if topic_divs:
                print(f"ë°©ë²•1: topictitle í´ë˜ìŠ¤ë¡œ {len(topic_divs)}ê°œ ë°œê²¬")
                for topic_div in topic_divs:
                    parent = topic_div.parent
                    if parent:
                        items.append(parent)
            
            # ë°©ë²• 2: ë§í¬ê°€ topic?id= íŒ¨í„´ì¸ ê²ƒë“¤ ì°¾ê¸° (ìƒëŒ€ê²½ë¡œ)
            if not items:
                all_links = soup.find_all('a', href=True)
                topic_links = [link for link in all_links if link.get('href', '').startswith('topic?id=')]
                print(f"ë°©ë²•2: topic ë§í¬ë¡œ {len(topic_links)}ê°œ ë°œê²¬")
                for link in topic_links[:20]:  # ìµœëŒ€ 20ê°œë¡œ ì œí•œ
                    items.append(link.parent if link.parent else link)
            
            print(f"  ğŸ” ì°¾ì€ ì•„ì´í…œ ìˆ˜: {len(items)}ê°œ")
            
            saved_count = 0
            for idx, item in enumerate(items, 1):
                
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ - ë” ìœ ì—°í•œ ë°©ì‹
                link_elem = None
                title = ""
                
                # ìš°ì„ ìˆœìœ„ ë³€ê²½: GeekNews ë‚´ë¶€ ë§í¬ë¥¼ ë¨¼ì € ì°¾ê¸°
                
                # ë°©ë²• 1: ì•„ì´í…œ ë‚´ì—ì„œ topic ë§í¬ ì°¾ê¸° - ëŒ“ê¸€ ë§í¬ê°€ ì•„ë‹Œ ë©”ì¸ ë§í¬ (ìµœìš°ì„ )
                topic_links = item.find_all('a', href=lambda x: x and 'topic?id=' in x)
                for t_link in topic_links:
                    href = t_link.get('href', '')
                    if 'go=comments' not in href:  # ëŒ“ê¸€ í˜ì´ì§€ê°€ ì•„ë‹Œ ë©”ì¸ í˜ì´ì§€ ë§í¬
                        link_elem = t_link
                        title = t_link.get_text(strip=True)
                        break
                
                # ë°©ë²• 2: topictitle divì—ì„œ ì œëª© í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ë§í¬ëŠ” ìœ„ì—ì„œ ì°¾ì€ ê²ƒ ì‚¬ìš©)
                if link_elem:
                    title_div = item.find('div', class_='topictitle')
                    if title_div:
                        title_link = title_div.find('a')
                        if title_link:
                            title = title_link.get_text(strip=True)  # ì™¸ë¶€ ë§í¬ì˜ ì œëª© í…ìŠ¤íŠ¸ ì‚¬ìš©
                
                # ë°©ë²• 3: ì§ì ‘ ë§í¬ì¸ ê²½ìš° (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
                if not link_elem and item.name == 'a':
                    href = item.get('href', '')
                    if 'topic?id=' in href and 'go=comments' not in href:
                        link_elem = item
                        title = item.get_text(strip=True)
                
                if not link_elem:
                    continue
                
                # ì œëª©ê³¼ URL ì¶”ì¶œ
                title = link_elem.get_text(strip=True)
                relative_url = link_elem.get('href', '')
                
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if relative_url.startswith('/topic?id='):
                    full_url = f"https://news.hada.io{relative_url}"
                elif relative_url.startswith('topic?id='):
                    full_url = f"https://news.hada.io/{relative_url}"
                elif relative_url.startswith('https://news.hada.io/topic?id='):
                    full_url = relative_url
                else:
                    continue  # ìœ íš¨í•˜ì§€ ì•Šì€ URLì€ ê±´ë„ˆëœ¸
                
                # í¬ë¡¤ë§ ì‹œê°„ ê¸°ë¡
                crawl_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # ëª¨ë“  ì•„ì´í…œ ì •ë³´ ì¶œë ¥
                print(f"\n[{idx}] {title[:50]}...")
                print(f"    URL: {full_url}")
                print(f"    í¬ë¡¤ë§ ì‹œê°„: {crawl_time}")
                
                # URL ì €ì¥ ì‹œë„ ë° ê²°ê³¼ í‘œì‹œ
                if save_url_to_ready(full_url):
                    saved_count += 1
                    print(f"    ê²°ê³¼: ìƒˆ URL ì €ì¥ ì™„ë£Œ")
                else:
                    print(f"    ê²°ê³¼: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URL (ê±´ë„ˆëœ€)")
                
                # 3ì´ˆ ëŒ€ê¸° (ë¡œë´‡ ë°°ì œ í‘œì¤€)
                time.sleep(3)
            
            print(f"\nìµœì‹ ê¸€ ì €ì¥ ê²°ê³¼: {saved_count}ê°œ ì‹ ê·œ ì €ì¥")
            
            # "í† í”½ ë” ë¶ˆëŸ¬ì˜¤ê¸°" ë²„íŠ¼ í´ë¦­ (1íšŒ)
            try:
                # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
                load_more_selectors = [
                    "div.next.commentTD > a",
                    ".next a",
                    "a:contains('í† í”½ ë”')",
                    "a[href*='page=']"
                ]
                
                load_more_button = None
                for selector in load_more_selectors:
                    try:
                        load_more_button = page.locator(selector)
                        if load_more_button.count() > 0:
                            break
                    except:
                        continue
                
                if load_more_button and load_more_button.count() > 0:
                    print("\n'í† í”½ ë” ë¶ˆëŸ¬ì˜¤ê¸°' í´ë¦­...")
                    time.sleep(3)  # í´ë¦­ ì „ ëŒ€ê¸°
                    load_more_button.click()
                    time.sleep(5)  # ë¡œë“œ ëŒ€ê¸°
                    
                    # ìƒˆë¡œ ë¡œë“œëœ ì½˜í…ì¸ ì—ì„œ ì•„ì´í…œ ìˆ˜ì§‘
                    content = page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # ë‹¤ì‹œ ì•„ì´í…œ ì°¾ê¸°
                    article = soup.select_one('body > main > article')
                    if article:
                        new_item_containers = article.select('div > div')
                        new_items = []
                        for container in new_item_containers:
                            if container.select_one('div.topictitle'):
                                new_items.append(container)
                        
                        print(f"ì „ì²´ ì•„ì´í…œ ìˆ˜: {len(new_items)}ê°œ (ê¸°ì¡´: {len(items)}ê°œ)")
                        
                        additional_saved = 0
                        # ê¸°ì¡´ ì•„ì´í…œ ìˆ˜ ì´í›„ì˜ ìƒˆ ì•„ì´í…œë§Œ ì²˜ë¦¬
                        for idx, item in enumerate(new_items[len(items):], len(items)+1):
                            # ë™ì¼í•œ ìœ ì—°í•œ ë§í¬ ì°¾ê¸° ë¡œì§ ì ìš©
                            link_elem = None
                            title_div = item.find('div', class_='topictitle')
                            if title_div:
                                link_elem = title_div.find('a')
                            
                            if not link_elem and item.name == 'a':
                                link_elem = item
                                
                            if not link_elem:
                                # ëŒ“ê¸€ì´ ì•„ë‹Œ ë©”ì¸ topic ë§í¬ë¥¼ ì°¾ê¸°
                                topic_links = item.find_all('a', href=lambda x: x and 'topic?id=' in x)
                                for t_link in topic_links:
                                    href = t_link.get('href', '')
                                    if 'go=comments' not in href:  # ëŒ“ê¸€ í˜ì´ì§€ê°€ ì•„ë‹Œ ë©”ì¸ í˜ì´ì§€ ë§í¬
                                        link_elem = t_link
                                        break
                            
                            if not link_elem:
                                continue
                            
                            title = link_elem.get_text(strip=True)
                            relative_url = link_elem.get('href', '')
                            
                            if relative_url.startswith('/topic?id='):
                                full_url = f"https://news.hada.io{relative_url}"
                            elif relative_url.startswith('topic?id='):
                                full_url = f"https://news.hada.io/{relative_url}"
                            elif relative_url.startswith('https://news.hada.io/topic?id='):
                                full_url = relative_url
                            else:
                                continue
                                
                            crawl_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            print(f"\n[ì¶”ê°€-{idx}] {title[:50]}...")
                            print(f"    URL: {full_url}")
                            print(f"    í¬ë¡¤ë§ ì‹œê°„: {crawl_time}")
                            
                            if save_url_to_ready(full_url):
                                additional_saved += 1
                                print(f"    ê²°ê³¼: ìƒˆ URL ì €ì¥ ì™„ë£Œ")
                            else:
                                print(f"    ê²°ê³¼: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” URL (ê±´ë„ˆëœ€)")
                            time.sleep(3)
                        
                        print(f"\nì¶”ê°€ ì €ì¥: {additional_saved}ê°œ")
                else:
                    print("'í† í”½ ë” ë¶ˆëŸ¬ì˜¤ê¸°' ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
            except Exception as e:
                print(f"ë”ë³´ê¸° í´ë¦­ ì˜¤ë¥˜: {e}")
                
        except Exception as e:
            print(f"ìµœì‹ ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            
        finally:
            try:
                browser.close()
            except:
                pass  # ë¸Œë¼ìš°ì €ê°€ ì´ë¯¸ ë‹«í˜€ìˆì–´ë„ ë¬´ì‹œ

def process_ready_urls():
    """gn_scrap_readyì—ì„œ ì²˜ë¦¬ ëŒ€ê¸°ì¤‘ì¸ URLë“¤ì„ ìˆ˜ì§‘í•˜ì—¬ gn_masterì— ì €ì¥í•©ë‹ˆë‹¤."""
    
    print("\n2ë‹¨ê³„: ì½˜í…ì¸  ìˆ˜ì§‘ ì‹œì‘...")
    
    try:
        # ì²˜ë¦¬ ëŒ€ê¸°ì¤‘ì¸ URLë“¤ ì¡°íšŒ (status = '0')
        cur.execute("SELECT seq_no, source_url FROM gn_scrap_ready WHERE status = '0' ORDER BY seq_no LIMIT 10")
        ready_items = cur.fetchall()
        
        if not ready_items:
            print("ì²˜ë¦¬í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        print(f"ì²˜ë¦¬í•  URL ê°œìˆ˜: {len(ready_items)}ê°œ")
        
        with sync_playwright() as p:
            browser = p.firefox.launch(headless=True)
            page = browser.new_page()
            
            processed_count = 0
            for ready_seq_no, url in ready_items:
                try:
                    print(f"\nì²˜ë¦¬ ì¤‘: {url}")
                    
                    # ìƒíƒœë¥¼ 'ìˆ˜ì§‘ì¤‘'ìœ¼ë¡œ ë³€ê²½
                    cur.execute("UPDATE gn_scrap_ready SET status = '2', update_dt = %s WHERE seq_no = %s",
                               (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), ready_seq_no))
                    conn.commit()
                    
                    # í˜ì´ì§€ ë¡œë“œ
                    page.goto(url, timeout=30000)
                    time.sleep(3)  # ë¡œë´‡ ë°°ì œ í‘œì¤€
                    
                    # í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                    content = page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # topic_id ì¶”ì¶œ (URLì—ì„œ)
                    import re
                    topic_match = re.search(r'topic\?id=(\d+)', url)
                    topic_id = topic_match.group(1) if topic_match else None
                    
                    # ì½˜í…ì¸  ì¶”ì¶œ
                    news_title = ""
                    news_desc = ""
                    topic_info = ""
                    external_url = ""
                    topic_url_domain = ""
                    author = ""
                    vote_count = 0
                    comment_count = 0
                    
                    # ì‹¤ì œ ì œëª© ì¶”ì¶œ - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
                    news_title = ""
                    
                    # ë°©ë²• 1: #tr1 > h1
                    main_title_elem = soup.select_one('#tr1 > h1')
                    if main_title_elem:
                        news_title = main_title_elem.get_text(strip=True)
                    
                    # ë°©ë²• 2: #tr1 ì „ì²´
                    if not news_title:
                        tr1_elem = soup.select_one('#tr1')
                        if tr1_elem:
                            news_title = tr1_elem.get_text(strip=True)
                    
                    # ë°©ë²• 3: h1 íƒœê·¸ ì „ì²´ ê²€ìƒ‰
                    if not news_title:
                        h1_elem = soup.find('h1')
                        if h1_elem:
                            news_title = h1_elem.get_text(strip=True)
                    
                    # ë°©ë²• 4: title íƒœê·¸
                    if not news_title:
                        title_elem = soup.find('title')
                        if title_elem:
                            title_text = title_elem.get_text(strip=True)
                            if " - GeekNews" in title_text:
                                news_title = title_text.replace(" - GeekNews", "")
                            else:
                                news_title = title_text
                    
                    # ì œëª©ì´ ì—¬ì „íˆ ì—†ìœ¼ë©´ URLì—ì„œ topic_id ì‚¬ìš©
                    if not news_title:
                        news_title = f"Topic {topic_id}"
                    
                    # ì™¸ë¶€ ë§í¬ ì •ë³´ (div.topictitle)
                    title_div = soup.find('div', class_='topictitle')
                    if title_div:
                        # ì™¸ë¶€ ë§í¬
                        external_links = title_div.find_all('a')
                        for link in external_links:
                            href = link.get('href', '')
                            if href.startswith('http') or (not href.startswith('topic?id=') and not href.startswith('/topic?id=')):
                                external_url = href
                                break
                        
                        # ë„ë©”ì¸ ì •ë³´ (span íƒœê·¸)
                        topicurl_span = title_div.find('span')
                        if topicurl_span:
                            topic_url_domain = topicurl_span.get_text(strip=True)
                    
                    # ì„¤ëª… (div.topicdesc)
                    desc_div = soup.find('div', class_='topicdesc')
                    if desc_div:
                        # topicdesc ë‚´ì˜ GeekNews ë‚´ë¶€ ë§í¬ ì°¾ê¸° (class="c99 breakall")
                        desc_links = desc_div.find_all('a', class_='c99')
                        if desc_links:
                            news_desc = desc_links[0].get_text(strip=True)
                        else:
                            news_desc = desc_div.get_text(strip=True)
                    
                    # í† í”½ ì •ë³´ (div.topicinfo)
                    info_div = soup.find('div', class_='topicinfo')
                    if info_div:
                        # ì „ì²´ topicinfo í…ìŠ¤íŠ¸
                        topic_info = info_div.get_text(strip=True)
                        
                        # ì‘ì„±ì ì¶”ì¶œ (/user?id= íŒ¨í„´)
                        author_links = info_div.find_all('a')
                        for link in author_links:
                            if '/user?id=' in link.get('href', ''):
                                author = link.get_text(strip=True)
                                break
                        
                        # ì‹œê°„ ì •ë³´ ì¶”ì¶œ ("3ì‹œê°„ì „" ê°™ì€ í…ìŠ¤íŠ¸)
                        import re
                        time_pattern = r'(\d+(?:ë¶„|ì‹œê°„|ì¼)ì „)'
                        time_match = re.search(time_pattern, topic_info)
                        if time_match:
                            news_pub_date = time_match.group(1)
                        else:
                            news_pub_date = ""
                    
                    # íˆ¬í‘œ ìˆ˜ (div.votenum)
                    vote_div = soup.find('div', class_='votenum')
                    if vote_div:
                        try:
                            vote_count = int(vote_div.get_text(strip=True))
                        except:
                            vote_count = 0
                    else:
                        vote_count = 0
                    
                    print(f"    ì œëª©: {news_title[:50]}...")
                    print(f"    ì„¤ëª…: {news_desc[:50]}...")
                    print(f"    ì‘ì„±ì: {author}")
                    print(f"    íˆ¬í‘œìˆ˜: {vote_count}")
                    if topic_url_domain:
                        print(f"    ë„ë©”ì¸: {topic_url_domain}")
                    if news_pub_date:
                        print(f"    ì‘ì„±ì‹œê°„: {news_pub_date}")
                    
                    # gn_masterì— ì €ì¥ (topic_info í•„ë“œì— ë„ë©”ì¸ ì •ë³´ë„ í¬í•¨)
                    full_topic_info = topic_info
                    if topic_url_domain:
                        full_topic_info += f" | ë„ë©”ì¸: {topic_url_domain}"
                    
                    crawl_dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    cur.execute("""
                        INSERT INTO gn_master 
                        (ready_seq_no, topic_id, news_url, external_url, news_title, news_desc, 
                         topic_info, author, vote_count, comment_count, news_pub_date, crawl_dt)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        ON DUPLICATE KEY UPDATE
                        news_title = VALUES(news_title),
                        news_desc = VALUES(news_desc),
                        topic_info = VALUES(topic_info),
                        author = VALUES(author),
                        vote_count = VALUES(vote_count),
                        news_pub_date = VALUES(news_pub_date),
                        crawl_dt = VALUES(crawl_dt)
                    """, (ready_seq_no, topic_id, url, external_url, news_title, news_desc,
                          full_topic_info, author, vote_count, comment_count, news_pub_date, crawl_dt))
                    
                    # ìƒíƒœë¥¼ 'ìˆ˜ì§‘ì™„ë£Œ'ë¡œ ë³€ê²½
                    cur.execute("UPDATE gn_scrap_ready SET status = '9', update_dt = %s WHERE seq_no = %s",
                               (crawl_dt, ready_seq_no))
                    conn.commit()
                    
                    processed_count += 1
                    print(f"    ì €ì¥ ì™„ë£Œ!")
                    
                except Exception as e:
                    print(f"    ì˜¤ë¥˜: {e}")
                    # ìƒíƒœë¥¼ 'ì‹¤íŒ¨'ë¡œ ë³€ê²½
                    cur.execute("UPDATE gn_scrap_ready SET status = '5', error_msg = %s, update_dt = %s WHERE seq_no = %s",
                               (str(e)[:500], datetime.now().strftime('%Y-%m-%d %H:%M:%S'), ready_seq_no))
                    conn.commit()
                    
                time.sleep(3)  # ë¡œë´‡ ë°°ì œ í‘œì¤€
            
            print(f"\nì½˜í…ì¸  ìˆ˜ì§‘ ì™„ë£Œ: {processed_count}ê°œ ì²˜ë¦¬")
            browser.close()
            
    except Exception as e:
        print(f"ì½˜í…ì¸  ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

def scrape_comments():
    """ëŒ“ê¸€ í˜ì´ì§€ì—ì„œ ì•„ì´í…œ URLë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    
    print("\nëŒ“ê¸€ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...")
    
    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        browser = p.firefox.launch(headless=True)
        page = browser.new_page()
        
        try:
            # ëŒ“ê¸€ í˜ì´ì§€ë¡œ ì´ë™
            print("í˜ì´ì§€ ë¡œë“œ ì¤‘: https://news.hada.io/comments")
            page.goto("https://news.hada.io/comments", timeout=30000)
            time.sleep(3)  # ë¡œë´‡ ë°°ì œ í‘œì¤€ ì¤€ìˆ˜
            
            # í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            content = page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # ëŒ“ê¸€ ì°¾ê¸° (idê°€ cidë¡œ ì‹œì‘í•˜ëŠ” div)
            comments = soup.find_all('div', id=lambda x: x and x.startswith('cid'))
            print(f"ì°¾ì€ ëŒ“ê¸€ ìˆ˜: {len(comments)}ê°œ")
            
            saved_count = 0
            unique_urls = set()  # ì¤‘ë³µ ì œê±°ìš©
            
            for idx, comment in enumerate(comments, 1):
                # ëŒ“ê¸€ ì •ë³´ ì˜ì—­ ì°¾ê¸°
                comment_info = comment.find('div', class_='commentinfo')
                if not comment_info:
                    continue
                
                # ëŒ“ê¸€ì´ ë‹¬ë¦° ì•„ì´í…œì˜ ë§í¬ ì°¾ê¸°
                # commentinfo ì•ˆì˜ ëª¨ë“  ë§í¬ë¥¼ í™•ì¸
                links = comment_info.find_all('a')
                item_url = None
                
                for link in links:
                    href = link.get('href', '')
                    # topic?id= íŒ¨í„´ì„ ì°¾ìŒ
                    if '/topic?id=' in href:
                        if href.startswith('/'):
                            item_url = f"https://news.hada.io{href}"
                        else:
                            item_url = href
                        break
                
                if not item_url:
                    continue
                
                # ëŒ“ê¸€ ì‹œê°„ ì •ë³´ (ë³´í†µ ë‘ ë²ˆì§¸ ë§í¬)
                time_elem = links[1] if len(links) > 1 else None
                comment_time = time_elem.get_text(strip=True) if time_elem else ""
                
                # í¬ë¡¤ë§ ì‹œê°„
                crawl_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # ì¤‘ë³µ ì œê±°
                if item_url not in unique_urls and '/topic?id=' in item_url:
                    unique_urls.add(item_url)
                    
                    # ëŒ“ê¸€ ë‚´ìš© ì¼ë¶€ ì¶”ì¶œ
                    comment_text = comment.find('div', class_='comment')
                    comment_preview = comment_text.get_text(strip=True)[:50] + "..." if comment_text else ""
                    
                    print(f"\n[ëŒ“ê¸€-{idx}] {item_url}")
                    print(f"    ëŒ“ê¸€ ì‹œê°„: {comment_time}")
                    print(f"    ëŒ“ê¸€ ë‚´ìš©: {comment_preview}")
                    print(f"    í¬ë¡¤ë§ ì‹œê°„: {crawl_time}")
                    
                    # URL ì €ì¥
                    if save_url_to_ready(item_url):
                        saved_count += 1
                    
                    time.sleep(3)  # ë¡œë´‡ ë°°ì œ í‘œì¤€
            
            print(f"\nëŒ“ê¸€ ì•„ì´í…œ ì €ì¥ ê²°ê³¼: {saved_count}ê°œ ì‹ ê·œ ì €ì¥")
            
        except Exception as e:
            print(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            
        finally:
            try:
                browser.close()
            except:
                pass  # ë¸Œë¼ìš°ì €ê°€ ì´ë¯¸ ë‹«í˜€ìˆì–´ë„ ë¬´ì‹œ

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print(f"\n{'='*60}")
    print(f"GeekNews í¬ë¡¤ë§ ì‹œì‘: {datetime.now()}")
    print(f"{'='*60}")
    
    try:
        # 1. ìµœì‹ ê¸€ ìˆ˜ì§‘
        scrape_latest_items()
        
        # 2. ëŒ“ê¸€ í˜ì´ì§€ ìˆ˜ì§‘
        scrape_comments()
        
        # 3. ì½˜í…ì¸  ìˆ˜ì§‘ (2ë‹¨ê³„)
        process_ready_urls()
        
        print(f"\n{'='*60}")
        print(f"í¬ë¡¤ë§ ì™„ë£Œ: {datetime.now()}")
        print(f"{'='*60}\n")
        
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
        cur.close()
        conn.close()
        print("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")

if __name__ == "__main__":
    main()